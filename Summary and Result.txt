======================================== Project Summary: Predicting Seoul Bike Sharing Demand
This project focuses on accurately predicting the hourly demand for rental bikes in Seoul's bike-sharing system. By leveraging a public dataset, the study aims to provide actionable insights for a bike rental company to optimize resource allocation, enhance user experience, and improve operational efficiency.

METHODOLOGY
The project followed the Cross-Industry Standard Process for Data Mining (CRISP-DM) framework, ensuring a structured approach from data understanding to deployment.

Data & Preprocessing:
The analysis utilized the "Seoul Bike Sharing Demand" dataset from the UCI Machine Learning Repository. This dataset contains 8,760 hourly observations with 14 features, including temporal information (date, hour) and weather conditions (temperature, humidity, wind speed, etc.).

Initial data exploration (EDA) revealed a right-skewed distribution for the target variable (Rented Bike Count) and strong correlations between certain features, such as Temperature and Dew Point Temperature. The data was cleaned, and new time-based features like "Day of the Week" and "isWeekend" were engineered.

Feature Engineering & Dataset Creation:
To thoroughly test the models, several distinct datasets were created based on different feature engineering strategies:

Encoded Dataset: The baseline dataset with all features, including one-hot encoded categorical variables.

No FD Dataset: Removed non-functioning days, as no bikes are rented on these days.

SD No FD Dataset: An extension of the "No FD" set, including additional special holidays.

No DEW Dataset: Addressed multicollinearity by removing the 'Dew Point Temperature' feature.

No Weak Dataset: Excluded features with a low correlation (< 0.1) to the target variable.

No FD DEW Dataset: A combination of the "No FD" and "No DEW" datasets.

Modeling:
A diverse range of machine learning algorithms was implemented to predict bike rental demand. Each model was trained and evaluated on all six datasets. The models fall into three main categories:

Linear Models: Lasso, Ridge, and ElasticNet Regression.

Tree-Based Models: Random Forest, Conditional Inference Trees (CI Tree), and XGBoost.

Other Models: K-Nearest Neighbors (KNN) and Support Vector Machine (SVM) Regressor.

Hyperparameter tuning, including Grid Search and Bayesian Optimization, was performed to optimize the performance of each model.

RESULTS & COMPARISON
Model performance was primarily evaluated using the R-squared (R^2) metric, which indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). The list below shows the best R^2 score achieved by each model across all datasets.

Model: XGBoost
Best Performing Dataset: Encoded Dataset
Best R-squared (R^2) Score: 0.9984

Model: Random Forest
Best Performing Dataset: SD No FD Dataset
Best R-squared (R^2) Score: 0.9381

Model: Conditional Inference Tree
Best Performing Dataset: No FD Dataset
Best R-squared (R^2) Score: 0.9240

Model: SVM Regressor
Best Performing Dataset: Encoded Dataset
Best R-squared (R^2) Score: 0.9239

Model: K-Nearest Neighbors (KNN)
Best Performing Dataset: SD No FD Dataset
Best R-squared (R^2) Score: 0.8132

Model: Lasso Regression
Best Performing Dataset: Encoded Dataset
Best R-squared (R^2) Score: 0.6284

Model: Ridge Regression
Best Performing Dataset: No FD Dataset
Best R-squared (R^2) Score: 0.6206

Model: ElasticNet Regression
Best Performing Dataset: No FD Dataset
Best R-squared (R^2) Score: 0.6206

CONCLUSION
The analysis demonstrates that tree-based ensemble models significantly outperform linear models and other algorithms for this prediction task.

Top Performer: The XGBoost model achieved a near-perfect R^2 score, indicating its exceptional ability to capture the complex, non-linear relationships within the data.

Strong Contenders: Random Forest, CI Tree, and SVM also delivered excellent results, with R^2 scores consistently above 0.92.

Dataset Impact: The "Encoded Dataset" and the "No FD Dataset" generally provided the best foundation for the top-performing models, suggesting that retaining most features is beneficial. The removal of weakly correlated features notably degraded performance across almost all models.

In conclusion, for predicting bike rental demand, advanced ensemble methods like XGBoost are the most effective. The comprehensive feature engineering and comparative model analysis provide a robust framework for a bike-sharing company to forecast demand accurately.
